---
title: "Simulation Bootcamp"
author: ""
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: yes
    toc_depth: 3
    toc_float: true
    number_sections: true
    code_folding: hide 
    theme: readable
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  cache = T
)
```

```{r, warning=FALSE, message=FALSE}
library(tidyverse)
```
# Task 1

## Getting into the basics

### Data generation and simple tests

1.1. Simulate 100 realizations of a normally distributed variable with mean 4 and standard deviation 2 (population a). Save the results in the vector a.

# Task 1.1
mean.a <- 4
sd.a <- 2
mean.b <- 4.8
sd.b <- 2

# 1.1
a <- rnorm(100, mean.a, sd.a)

# 1.2
b <- rnorm(100, mean.b, sd.b)

# 1.3
t.test(a, b)

1.2. Simulate 100 realizations of a normally distributed variable with mean 4.8 and standard deviation 2 (population b). Save the results in the vector b.

```{r}
#set.seed(100)
a <- rnorm(100, mean = 4, sd = 2)
b <- rnorm(100, mean = 4.8, sd = 2)
```

1.3. Use a t-test to test the null hypothesis that the mean of population a is the same as the mean of population b. What is the interpretation of the p-value? What happens when you re-sample the vectors a and b?

```{r}
t.test(a, b)
wilcox.test(a, b)
```


1.4. Simulate 500 realizations of a Bernoulli distributed random variable with success probability p = 0.15 (population c). Save the results in the vector c. (Hint: Use Binomial Distribution).

1.5. Simulate 500 realizations of a Bernoulli distributed random variable with success probability p = 0.20 (population d). Save the results in the vector d.

```{r}
n = 500
p = 0.15
c <- sample(0:1, size = n, replace = TRUE, prob = c(p, 1-p))
p = 0.2
d <- sample(0:1, size = n, replace = TRUE, prob = c(p, 1-p))
```

1.6. Use a $\chi^{2}$-test to test the null hypothesis that the success probability of population c is the same as the success probability of population d. What is the p-value? What happens when you re-sample the vectors c and d? Which other test could be used to test this null hypothesis?

```{r}
chisq.test(c, d)$p.value
```

### Repetitions

2.1. The workflows from 1.1-1.3 and 1.4-1.6 assumed that there was indeed an underlying difference in populations a vs b and c vs d. What happens if you simulate twice from the same distribution and use a statistical test to compare the mean/success proportion? If you repeat this many times and average over the test decisions, which quantity are you estimating?


# 2.1, (1.1-1.3)

```{r}
# 2.1, (1.1-1.3)
x <- 10000
pval <- numeric()
mean.a <- 4
mean.b <- 4.8
sd.a <- sd.b <- 2

for (i in 1:x){
a1 <- rnorm(100, mean.a, sd.a)
a2 <- rnorm(100, mean.a, sd.a)

pval[i] <- t.test(a1, a2)$p.val
}

# reject H_0 if p-value < alpha
alpha <- 0.05
rejection <- pval < 0.05

mean(rejection)
```

# result is exactly the Significance level alpha
# probability of wrongly rejecting H0





```{r}
# for 1.4-1.6
nsim <- 1000
p <-  0.15
p1 <- 0.2
c_matrx <- d_matrx <- matrix(numeric(n * nsim), nrow = nsim)
res <- numeric(nsim)
est <- matrix(numeric(2 * nsim), nrow = nsim)
for (i in 1:nsim) {
  c_matrx[i, ] <- sample(0:1, size = n, replace = TRUE, prob = c(1-p, p))
  d_matrx[i, ] <- sample(0:1, size = n, replace = TRUE, prob = c(1-p1, p1))
  res[i ] <- chisq.test(c_matrx[i, ], d_matrx[i, ])$p.value
}
est[, 1] <- rowMeans(c_matrx)
est[, 2] <- rowMeans(d_matrx)

```

2.2. The workflows from 1.1-1.3 and 1.4-1.6 could correspond to a single clinical trial, e.g. testing whether two patient populations differ with respect to a treatment outcome, whereby the realizations correspond to different observed patient outcomes. As a biostatistician, you are interested in investigating how often on average the statistical test would wrongly not reject the null hypothesis (type two error). How can you do this (and then calculate the power)?


# 2.2, 1.1-1.3

```{r}
x <- 10000
pval <- numeric()

for (i in 1:x){
  a2 <- rnorm(100, mean.a, sd.a)
  b2 <- rnorm(100, mean.b, sd.b)
  
  pval[i] <- t.test(a2, b2)$p.val
}

# reject H_0 if p-value < alpha
alpha <- 0.05
rejection <- pval < 0.05

power <- mean(rejection)
# power = correctly rejecting H_0
type2_err <- 1- power
# type2_err = wrongly not rejecting H_0
```




```{r}
# for 1.4-1.6
res_h1 <- res > 0.05
# type two error
mean(res_h1)
# power
1 - mean(res_h1)

```

2.3. For tasks 1.1-1.3 we have a power of 80% and a type 1 error of 5%, which is what is usually required for a clinical study. Assume that the assumptions of a t-test are violated with respect to the normality of the data, i.e. both populations follow a log-normal distribution with the above means and standard deviations. What happens with the power and type 1 error?

2.4. For tasks 1.1-1.3, what happens with the power and type 1 error if we use a Wilcoxon test instead of a t Test?

```{r}
# Power simulation normal distribution
decision_t <- rep(NA, 1000)
decision_w <- rep(NA, 1000)

for (i in 1:1000) {
  a <- rnorm(100, mean = 4, sd = 2)
  b <- rnorm(100, mean = 4.8, sd = 2)
  
  t <- t.test(a, b)
  w <- wilcox.test(a, b)
  decision_t[i] <- (t$p.value < 0.05)
  decision_w[i] <- (w$p.value < 0.05)
}

power_t <- mean(decision_t)
power_w <- mean(decision_w)

power_t; power_w
```

```{r}
# Type I error simulation normal distribution
decision_t <- rep(NA, 1000)
decision_w <- rep(NA, 1000)

for (i in 1:1000) {
  a <- rnorm(100, mean = 4, sd = 2)
  b <- rnorm(100, mean = 4, sd = 2)
  
  t <- t.test(a, b)
  w <- wilcox.test(a, b)
  decision_t[i] <- (t$p.value < 0.05)
  decision_w[i] <- (w$p.value < 0.05)
}

alpha_t <- mean(decision_t)
alpha_w <- mean(decision_w)

alpha_t; alpha_w
```



```{r}
# Power simulation log-normal distribution
decision_t <- rep(NA, 1000)
decision_w <- rep(NA, 1000)

for (i in 1:1000) {
  a <- rlnorm(100, meanlog = 4, sdlog = 2)
  b <- rlnorm(100, meanlog = 4.8, sdlog = 2)
  
  t <- t.test(a, b)
  w <- wilcox.test(a, b)
  decision_t[i] <- (t$p.value < 0.05)
  decision_w[i] <- (w$p.value < 0.05)
}

power_t <- mean(decision_t)
power_w <- mean(decision_w)

power_t; power_w
```



```{r}
# Type I error simulation log-normal distribution
decision_t <- rep(NA, 1000)
decision_w <- rep(NA, 1000)

for (i in 1:1000) {
  a <- rlnorm(100, meanlog = 4, sdlog = 2)
  b <- rlnorm(100, meanlog = 4, sdlog = 2)
  
  t <- t.test(a, b)
  w <- wilcox.test(a, b)
  decision_t[i] <- (t$p.value < 0.05)
  decision_w[i] <- (w$p.value < 0.05)
}

alpha_t <- mean(decision_t)
alpha_w <- mean(decision_w)

alpha_t; alpha_w
```


# Task 2

## Moving towards advanced concepts

### Custom Test Functions

3.1. Write a function z_test() that takes as input parameters 1) two vectors of observations x and y, 2) an assumed common standard deviation and 3) the mean difference corresponding to the null hypothesis (delta) (two sample z-Test formula can be found online e.g. here). The output should be a list including the test statistic and the p-value.

```{r}
z_test <- function(x, y, sd=1, delta=0){
  
  n_x <- length(x)
  n_y <- length(y)
  
  # calculate the z-statistic
  z_stat <- (mean(x) - mean(y) - delta) / 
    sqrt(sd^2/n_x + sd^2/n_y)
  
  
  pvalue <- 1-pnorm(abs(z_stat))
  #decision <- (pvalue < 0.05)
  
  return(list(z_stat=z_stat, p.value=pvalue))
}
```

3.2. Repeat the exercises from 1.1-1.3 now using the z_test() function and the true standard deviation. Compare the results with those obtained previously.

```{r}
a <- rnorm(100, mean = 4, sd = 2)
b <- rnorm(100, mean = 4.8, sd = 2)
```

```{r}
z_test(a, b, sd=2)
t.test(a, b)
```

### Runtime considerations

4.1. What are possible ways in R to measure the runtime of a particular program or line of code? Choose one option from this article and measure the runtime of task 3.2, whereby you run multiple repetitions similar to the tasks in section 2. Try to increase the number of repetitions, until you achieve a runtime of 5 seconds.

```{r}
# Power simulation log-normal distribution
n_sim <- 200000
decision_z <- rep(NA, n_sim)

start_time <- Sys.time()
for (i in 1:n_sim) {
  a <- rnorm(100, mean = 4, sd = 2)
  b <- rnorm(100, mean = 4.8, sd = 2)
  
  z <- z_test(a, b, sd = 2)
  decision_z[i] <- (z$p.value < 0.05)
}
power_z <- mean(decision_z)
runtime <- Sys.time() - start_time

power_z; runtime
```

4.2. For tasks 1.1 and 1.2 you know the distribution of the two sample means (hint). Instead of simulating the observations and passing them to the z_test() function, you can also simulate the means and implement the z-Test directly (i.e. a numerical operation that returns the p-value). Use the same number of repetitions as in 4.1. Did the runtime change? Did the simulated power change? What changes, if you increase the number of observations from 100 to 10.000? (Hint: Make sure to reduce the number of repetitions.)

```{r}
z_test_new <- function(mean_x, mean_y, n_x, n_y, sd=1, delta=0){
  
  # calculate the z-statistic
  z_stat <- (mean_x - mean_y - delta) / 
    sqrt(sd^2/n_x + sd^2/n_y)
  
  pvalue <- 1-pnorm(abs(z_stat))
  #decision <- (pvalue < 0.05)
  
  return(list(z_stat=z_stat, p.value=pvalue))
}
```

```{r}
# Power simulation log-normal distribution
n_sim <- 200000
decision_z <- rep(NA, n_sim)
n_x <- 10000
n_y <- 10000

start_time <- Sys.time()
for (i in 1:n_sim) {
  
  mean_x <- rnorm(1, mean = 4, sd=2/sqrt(n_x))
  mean_y <- rnorm(1, mean = 4.8, sd=2/sqrt(n_y))
  
  z <- z_test_new(mean_x, mean_y, n_x, n_y, sd = 2)
  decision_z[i] <- (z$p.value < 0.05)
}
power_z <- mean(decision_z)
runtime <- Sys.time() - start_time

power_z; runtime
```

# The ultimate challenge

### Full Simulation Function

5.1. Based on everything you have programmed so far, write a simulation function that calculates the rejection probability of a particular set of assumptions (sample sizes, means, standard deviations). This function takes the following input parameters: 1) Sample Sizes in both groups (assumed to be equal), 2) Means and Standard Deviations of both groups, 3) Choice of t-test, z-test or Wilcoxon test, 4) Number of simulation repetitions. The output should be the rejection probability of the statistical test.

```{r}
run_trial <- function(n, mean_x, mean_y, sd_x, sd_y, test, alpha=0.05, n_sim, sim_alpha=TRUE){
  
  decision <- rep(NA, n_sim)
  for (i in 1:n_sim) {
    x <- rnorm(n, mean = mean_x, sd = sd_x)
    y <- rnorm(n, mean = mean_y, sd = sd_y)
    
    if(test=="t.test"){
      test_res <- t.test(x, y)
    }
    
    if(test=="wilcox.test"){
      test_res <- wilcox.test(x, y)
    }
    
    if(test=="z.test"){
      test_res <- z_test(x, y, sd=sd_x)
    }
    
    decision[i] <- (test_res$p.value < alpha)  
  }
  
  power <- mean(decision)
  
  if(sim_alpha){
    decision <- rep(NA, n_sim)
    for (i in 1:n_sim) {
      x <- rnorm(n, mean = mean_x, sd = sd_x)
      y <- rnorm(n, mean = mean_x, sd = sd_x)
      
      if(test=="t.test"){
        test_res <- t.test(x, y)
      }
      
      if(test=="wilcox.test"){
        test_res <- wilcox.test(x, y)
      }
      
      if(test=="z.test"){
        test_res <- z_test(x, y, sd=sd_x)
      }
      
      decision[i] <- (test_res$p.value < alpha)  
    }
    
    alpha <- mean(decision)
  } else {
    alpha = "not simulated"
  }
  return(list(power=power, alpha=alpha))
}
```


```{r}
run_trial(n=100, mean_x = 4, mean_y = 4.8, sd_x = 2, sd_y = 2, test = "wilcox.test", n_sim = 1000)
```

```{r}
sample_sizes <- seq(100, 2000, by=100)
df <- data.frame(power = rep(NA, length(sample_sizes)*3*3),
                 alpha = rep(NA, length(sample_sizes)*3*3),
                 sample_size = rep(sample_sizes, 3*3),
                 test = rep(c("t.test", "z.test", "wilcox.test"), each=length(sample_sizes)*3),
                 means = rep(c("4, 4.2", "4, 4.8", "4, 6"), each=length(sample_sizes), times=3),
                 mean_x = 4,
                 mean_y = rep(c(4.2, 4.8, 6), each=length(sample_sizes), times=3))

start_time <- Sys.time()
for (i in 1:nrow(df)) {
  res <- run_trial(n=df$sample_size[i], mean_x = df$mean_x[i], mean_y = df$mean_y[i], sd_x = 2, sd_y = 2, test = df$test[i], n_sim = 1000)
  df$power[i] <- res$power
  df$alpha[i] <- res$alpha
}

runtime_all <- Sys.time() - start_time
runtime_all
```

5.2. Visualize the impact of the sample size, assumed means and test procedure on the power and type 1 error. On the y-axis, both power and type 1 error should be plotted. On the x-axis you have the sample size, in the rows the test procedure and in the columns the assumed means. Fix the standard deviation to a sensible value. Use ggplot and the facet_grid() function.

```{r, fig.height=10, fig.width=7}
ggplot(df) +
  geom_point(aes(sample_size, power), color="darkred") +
  geom_line(aes(sample_size, power), color="darkred") +
  geom_point(aes(sample_size, alpha), color="darkblue") +
  geom_line(aes(sample_size, alpha), color="darkblue") +
  facet_grid(rows = vars(test), cols = vars(df$means)) +
  geom_hline(yintercept = 0.05) +
  geom_hline(yintercept = 0.8)
```















